---
title: "Local AI Plugin"
description: "Fully offline AI capabilities for ElizaOS"
---

# Local AI Plugin

The Local AI plugin provides completely offline AI capabilities using local models, perfect for privacy-sensitive applications or environments without internet access.

## Features

- **100% offline** - No internet connection required
- **Privacy-first** - Data never leaves your machine
- **No API keys** - Zero configuration needed
- **Multimodal** - Text, embeddings, vision, and speech

## Installation

```bash
elizaos plugins add @elizaos/plugin-local-ai
```

## Automatic Activation

Local AI serves as the ultimate fallback when no cloud providers are configured:

```typescript
// Automatically loads when these are not set:
// - GOOGLE_GENERATIVE_AI_API_KEY
// - OLLAMA_API_ENDPOINT  
// - OPENAI_API_KEY
```

## Configuration

### Environment Variables

```bash
# Optional configuration
LOCAL_AI_MODEL_PATH=/path/to/models
LOCAL_AI_THREADS=4
LOCAL_AI_CONTEXT_SIZE=4096
```

### Character Configuration

```json
{
  "name": "MyAgent",
  "plugins": ["@elizaos/plugin-local-ai"]
}
```

## Supported Operations

| Operation | Technology | Notes |
|-----------|------------|-------|
| TEXT_GENERATION | llama.cpp | Various model sizes |
| EMBEDDING | Local transformers | Sentence embeddings |
| VISION | Local vision models | Image description |
| SPEECH | Whisper + TTS | Transcription & synthesis |

## Model Management

The plugin automatically downloads required models on first use:

```typescript
// Models are cached in:
// ~/.eliza/models/
```

### Available Models

#### Text Generation
- Small: 1-3B parameter models
- Medium: 7B parameter models  
- Large: 13B+ parameter models

#### Embeddings
- Sentence transformers
- MiniLM variants

#### Vision
- BLIP for image captioning
- CLIP for image understanding

## Performance Optimization

### CPU Optimization
```bash
# Use more threads
LOCAL_AI_THREADS=8

# Enable AVX2 (if supported)
LOCAL_AI_USE_AVX2=true
```

### Memory Management
```bash
# Limit context size
LOCAL_AI_CONTEXT_SIZE=2048

# Use quantized models
LOCAL_AI_QUANTIZATION=q4_0
```

## Hardware Requirements

| Feature | Minimum RAM | Recommended |
|---------|-------------|-------------|
| Text (Small) | 4GB | 8GB |
| Text (Medium) | 8GB | 16GB |
| Embeddings | 2GB | 4GB |
| Vision | 4GB | 8GB |
| All Features | 16GB | 32GB |

## Common Use Cases

### 1. Development Environment
```json
{
  "plugins": ["@elizaos/plugin-local-ai"],
  "settings": {
    "local_ai": {
      "model_size": "small",
      "fast_mode": true
    }
  }
}
```

### 2. Privacy-Critical Applications
```json
{
  "plugins": ["@elizaos/plugin-local-ai"],
  "settings": {
    "local_ai": {
      "model_size": "large",
      "disable_telemetry": true
    }
  }
}
```

### 3. Offline Deployment
```json
{
  "plugins": ["@elizaos/plugin-local-ai"],
  "settings": {
    "local_ai": {
      "preload_models": true,
      "cache_responses": true
    }
  }
}
```

## Limitations

- Slower than cloud APIs
- Limited model selection
- Higher memory usage
- CPU-bound performance

## Troubleshooting

### Model Download Issues
```bash
# Clear model cache
rm -rf ~/.eliza/models/

# Download models manually
eliza download-models
```

### Performance Issues
1. Use smaller models
2. Enable quantization
3. Reduce context size
4. Add more RAM/CPU

## External Resources

- [Plugin Source](https://github.com/elizaos/eliza/tree/main/packages/plugin-local-ai)
- [Model Compatibility List](https://github.com/elizaos/eliza/wiki/local-ai-models)
- [Performance Tuning Guide](https://github.com/elizaos/eliza/wiki/local-ai-performance)