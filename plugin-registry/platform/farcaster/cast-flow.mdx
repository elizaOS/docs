---
title: "Cast Flow"
description: "Visual guide to understanding how the Farcaster plugin processes casts and interactions"
---

# Farcaster Cast Flow

## Overview

This document provides a visual and detailed explanation of how the Farcaster plugin processes casts, from initial receipt through evaluation, response generation, and posting.

## Cast Processing Pipeline

```mermaid
graph TD
    A[Neynar API Polling] --> B{Event Type}
    B -->|New Cast| C[Cast Processor]
    B -->|Reply| D[Reply Handler]
    B -->|Mention| E[Mention Handler]
    B -->|Timeline Update| F[Timeline Handler]
    
    C --> G[Content Analysis]
    D --> G
    E --> G
    
    G --> H{Should Respond?}
    H -->|Yes| I[Generate Response]
    H -->|No| J[Store & Skip]
    
    I --> K[Format Cast]
    K --> L[Neynar API Call]
    L --> M[Submit Cast]
    M --> N[Store Result]
    
    F --> O[Update Context]
    J --> O
    N --> O
```

## Detailed Flow Stages

### 1. Event Reception

The plugin polls the Neynar API for relevant events and interactions:

```typescript
// Neynar API polling for mentions and timeline
setInterval(async () => {
  const mentions = await neynarClient.fetchMentions({
    fid: agentFid,
    limit: 10
  });
  
  const timeline = await neynarClient.fetchTimeline({
    fid: agentFid,
    type: 'ForYou'
  });
  
  await processEvents(mentions, timeline);
}, FARCASTER_POLL_INTERVAL * 60000);
```

### 2. Event Classification

Events are classified and routed to appropriate handlers:

```mermaid
graph LR
    A[Incoming Event] --> B{Classification}
    B --> C[Direct Mention]
    B --> D[Channel Cast]
    B --> E[Reply Thread]
    B --> F[Timeline Cast]
    
    C --> G[Priority Queue]
    D --> H[Channel Handler]
    E --> I[Thread Handler]
    F --> J[Timeline Handler]
```

### 3. Content Analysis

Each cast undergoes multi-stage analysis:

```mermaid
graph TD
    A[Cast Content] --> B[Tokenization]
    B --> C[Sentiment Analysis]
    C --> D[Topic Extraction]
    D --> E[Context Building]
    E --> F[Relevance Scoring]
    F --> G{Score Threshold}
    G -->|High| H[Immediate Response]
    G -->|Medium| I[Queue for Response]
    G -->|Low| J[Monitor Only]
```

### 4. Response Decision Tree

```mermaid
graph TD
    A[Cast Received] --> B{Is Mention?}
    B -->|Yes| C[High Priority Response]
    B -->|No| D{Is Reply to Agent?}
    D -->|Yes| E[Continue Conversation]
    D -->|No| F{Contains Keywords?}
    F -->|Yes| G{Sentiment Check}
    F -->|No| H[No Response]
    G -->|Positive| I[Engage Positively]
    G -->|Negative| J[Careful Response]
    G -->|Neutral| K{Random Engagement}
    K -->|Yes| L[Generate Response]
    K -->|No| H
```

### 5. Response Generation

The response generation process:

```typescript
async function generateResponse(context: CastContext): Promise<string> {
  // 1. Build conversation history
  const thread = await getThreadContext(context.parentHash);
  
  // 2. Extract key topics
  const topics = extractTopics(context.text);
  
  // 3. Generate appropriate response
  const response = await llm.generate({
    system: character.personality,
    context: thread,
    topics: topics,
    maxLength: 320
  });
  
  // 4. Validate and format
  return formatCast(response);
}
```

### 6. Cast Composition

```mermaid
graph TD
    A[Generated Text] --> B{Length Check}
    B -->|Over 320| C[Truncate/Split]
    B -->|Under 320| D[Format Check]
    C --> D
    D --> E{Has Embeds?}
    E -->|Yes| F[Validate URLs]
    E -->|No| G[Add Metadata]
    F --> G
    G --> H{Channel Cast?}
    H -->|Yes| I[Add Channel Tag]
    H -->|No| J[Standard Cast]
    I --> K[Final Cast Object]
    J --> K
```

### 7. Cast Publishing via Neynar

```mermaid
graph LR
    A[Cast Object] --> B[Format Request]
    B --> C[Add Signer UUID]
    C --> D[Neynar API Call]
    D --> E{Response}
    E -->|Success| F[Store Cast Hash]
    E -->|Error| G[Retry Logic]
    G --> H[Exponential Backoff]
    H --> D
```

## Interaction Patterns

### Reply Chains

```mermaid
graph TD
    A[Original Cast] --> B[Agent Reply 1]
    B --> C[User Reply]
    C --> D[Agent Reply 2]
    D --> E[Thread Continuation]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#9ff,stroke:#333,stroke-width:2px
    style C fill:#ff9,stroke:#333,stroke-width:2px
    style D fill:#9ff,stroke:#333,stroke-width:2px
```

### Channel Participation

```mermaid
graph TD
    A[Monitor Channel] --> B{New Cast}
    B --> C[Evaluate Relevance]
    C --> D{Relevant?}
    D -->|Yes| E[Analyze Context]
    D -->|No| F[Skip]
    E --> G{Can Contribute?}
    G -->|Yes| H[Post to Channel]
    G -->|No| I[Like/Recast Only]
```

## Rate Limiting & Throttling

```mermaid
graph TD
    A[Action Request] --> B{Check Rate Limit}
    B -->|Under Limit| C[Execute Action]
    B -->|At Limit| D[Queue Action]
    D --> E[Wait Period]
    E --> F[Retry Queue]
    F --> B
    C --> G[Update Counter]
    G --> H[Reset Timer]
```

## Error Handling Flow

```mermaid
graph TD
    A[Cast Attempt] --> B{Success?}
    B -->|Yes| C[Complete]
    B -->|No| D{Error Type}
    D -->|Network| E[Retry with Backoff]
    D -->|Validation| F[Fix & Retry]
    D -->|Rate Limit| G[Queue for Later]
    D -->|Fatal| H[Log & Abandon]
    
    E --> I{Max Retries?}
    I -->|No| A
    I -->|Yes| H
    F --> A
    G --> J[Delayed Retry]
    J --> A
```

## Performance Metrics

### Processing Times

```mermaid
graph LR
    A[Event Receipt] -->|~50ms| B[Classification]
    B -->|~100ms| C[Analysis]
    C -->|~200ms| D[Response Gen]
    D -->|~50ms| E[Formatting]
    E -->|~100ms| F[Submission]
    F -->|~50ms| G[Confirmation]
```

### Throughput Management

```mermaid
graph TD
    A[Incoming Events] --> B[Event Queue]
    B --> C{Queue Size}
    C -->|Low| D[Process Immediately]
    C -->|Medium| E[Batch Process]
    C -->|High| F[Priority Filter]
    F --> G[Process High Priority]
    F --> H[Defer Low Priority]
```

## State Management

```mermaid
graph TD
    A[Plugin State] --> B[Active Conversations]
    A --> C[Pending Responses]
    A --> D[Rate Limit Status]
    A --> E[Neynar API Status]
    
    B --> F[Thread Contexts]
    C --> G[Response Queue]
    D --> H[Cooldown Timers]
    E --> I[API Health Check]
```

## Monitoring & Observability

```mermaid
graph TD
    A[Cast Activity] --> B[Metrics Collector]
    B --> C[Response Times]
    B --> D[Success Rates]
    B --> E[Engagement Metrics]
    B --> F[Error Rates]
    
    C --> G[Dashboard]
    D --> G
    E --> G
    F --> G
    
    G --> H[Alerts]
    H --> I[Auto-Scaling]
    H --> J[Manual Intervention]
```

## Best Practices

1. **Efficient Polling**: Use appropriate intervals to balance responsiveness and API rate limits
2. **Smart Caching**: Cache user profiles and recent casts to reduce Neynar API calls
3. **Graceful Degradation**: Handle API failures without losing queued responses
4. **Context Awareness**: Maintain conversation context across reply chains
5. **Rate Limit Respect**: Implement proper backoff strategies for Neynar API limits

## Debugging Cast Flow

Enable detailed logging to trace cast processing:

```typescript
// Enable debug mode
process.env.FARCASTER_DEBUG = 'true';

// Log each stage
runtime.on('farcaster:event', (event) => {
  console.log(`[${event.stage}]`, event.data);
});
```

## Summary

The Farcaster cast flow is designed to be:
- **Responsive**: Quick reaction to mentions and replies
- **Intelligent**: Context-aware response generation
- **Reliable**: Robust error handling and retry logic
- **Scalable**: Efficient queue management and rate limiting
- **Observable**: Comprehensive metrics and logging
